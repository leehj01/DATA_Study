{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_x , train_y), (test_x, test_y) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x[..., tf.newaxis]\n",
    "test_x = test_x[..., tf.newaxis]\n",
    "\n",
    "train_x = train_x / 255.\n",
    "test_x = test_x / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28,28,1)\n",
    "num_classes = 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "dropout_rate = 0.7\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input( input_shape )\n",
    "net = layers.Conv2D(32, (3,3), padding = 'SAME')(inputs)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.Conv2D(32, (3,3), padding = 'SAME')(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.MaxPooling2D(pool_size = (2,2))(net)\n",
    "net = layers.Dropout(0.5)(net)\n",
    "\n",
    "net = layers.Conv2D(64, (3,3), padding = 'SAME')(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.Conv2D(64, (3,3), padding = 'SAME')(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.MaxPooling2D(pool_size = (2,2))(net)\n",
    "net = layers.Dropout(0.5)(net)\n",
    "\n",
    "net = layers.Flatten()(net)\n",
    "net = layers.Dense(512)(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.Dropout(0.5)(net)\n",
    "net = layers.Dense(num_classes)(net)\n",
    "net = layers.Activation('softmax')(net)\n",
    "\n",
    "model = tf.keras.Model(inputs = inputs , outputs = net, name =\"Basic_CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model is the full model w/o custom layers\n",
    "model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate),\n",
    "                 loss = 'sparse_categorical_crossentropy',\n",
    "                 metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 127s 135ms/step - loss: 2.3019 - accuracy: 0.1110\n",
      "157/157 [==============================] - 3s 21ms/step - loss: 2.3011 - accuracy: 0.1135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.301063060760498, 0.11349999904632568]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(train_x, train_y, batch_size = batch_size , shuffle= True)\n",
    "\n",
    "model.evaluate(test_x , test_y, batch_size= batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets , transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "lr = 0.001\n",
    "momentum = 0.5\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "epochs = 1\n",
    "no_cuda = False\n",
    "log_interval = 100  # 100 번 돌때마다 log 를 보여주겠다 ! - tf 는 프로그래스바 제공해줌 torch 는 안줌.. 우리가 짜야함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,20,5,1)\n",
    "        self.conv2 = nn.Conv2d(20, 50 ,5 ,1)\n",
    "        self.fc1 = nn.Linear(4 *4 *50, 500)\n",
    "        self.fc2 = nn.Linear(500, 19)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x  = F.max_pool2d(x , 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x  = F.max_pool2d(x , 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x , dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "kwargs = {'num_workers' : 1 , 'pin_memory' : True} if use_cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('dataset', train = True , download = True ,\n",
    "                        transform = transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,),(0.3081,))\n",
    "                        ])),\n",
    "        batch_size = batch_size , shuffle= True , **kwargs)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('dataset', train = False  ,\n",
    "                        transform = transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,),(0.3081,))\n",
    "                        ])),\n",
    "        batch_size = batch_size , shuffle= True , **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = lr , momentum= momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch : 1 [0/60000 (0%)] Loss :2.929361\n",
      "Train Epoch : 1 [64/60000 (0%)] Loss :2.929743\n",
      "Train Epoch : 1 [128/60000 (0%)] Loss :2.941966\n",
      "Train Epoch : 1 [192/60000 (0%)] Loss :2.914491\n",
      "Train Epoch : 1 [512/60000 (1%)] Loss :2.898558\n",
      "Train Epoch : 1 [576/60000 (1%)] Loss :2.891137\n",
      "Train Epoch : 1 [640/60000 (1%)] Loss :2.902653\n",
      "Train Epoch : 1 [704/60000 (1%)] Loss :2.896774\n",
      "Train Epoch : 1 [1024/60000 (2%)] Loss :2.877575\n",
      "Train Epoch : 1 [1088/60000 (2%)] Loss :2.882554\n",
      "Train Epoch : 1 [1152/60000 (2%)] Loss :2.869534\n",
      "Train Epoch : 1 [1216/60000 (2%)] Loss :2.866179\n",
      "Train Epoch : 1 [1536/60000 (3%)] Loss :2.853128\n",
      "Train Epoch : 1 [1600/60000 (3%)] Loss :2.831231\n",
      "Train Epoch : 1 [1664/60000 (3%)] Loss :2.852648\n",
      "Train Epoch : 1 [1728/60000 (3%)] Loss :2.848146\n",
      "Train Epoch : 1 [8192/60000 (14%)] Loss :2.367702\n",
      "Train Epoch : 1 [8256/60000 (14%)] Loss :2.348148\n",
      "Train Epoch : 1 [8320/60000 (14%)] Loss :2.343766\n",
      "Train Epoch : 1 [8384/60000 (14%)] Loss :2.329055\n",
      "Train Epoch : 1 [8704/60000 (14%)] Loss :2.330286\n",
      "Train Epoch : 1 [8768/60000 (15%)] Loss :2.339823\n",
      "Train Epoch : 1 [8832/60000 (15%)] Loss :2.303710\n",
      "Train Epoch : 1 [8896/60000 (15%)] Loss :2.321517\n",
      "Train Epoch : 1 [9216/60000 (15%)] Loss :2.243490\n",
      "Train Epoch : 1 [9280/60000 (15%)] Loss :2.306308\n",
      "Train Epoch : 1 [9344/60000 (16%)] Loss :2.281514\n",
      "Train Epoch : 1 [9408/60000 (16%)] Loss :2.294282\n",
      "Train Epoch : 1 [9728/60000 (16%)] Loss :2.291732\n",
      "Train Epoch : 1 [9792/60000 (16%)] Loss :2.267797\n",
      "Train Epoch : 1 [9856/60000 (16%)] Loss :2.260710\n",
      "Train Epoch : 1 [9920/60000 (17%)] Loss :2.218317\n",
      "Train Epoch : 1 [16384/60000 (27%)] Loss :1.798951\n",
      "Train Epoch : 1 [16448/60000 (27%)] Loss :1.873965\n",
      "Train Epoch : 1 [16512/60000 (28%)] Loss :1.741688\n",
      "Train Epoch : 1 [16576/60000 (28%)] Loss :1.874588\n",
      "Train Epoch : 1 [16896/60000 (28%)] Loss :1.816632\n",
      "Train Epoch : 1 [16960/60000 (28%)] Loss :1.848178\n",
      "Train Epoch : 1 [17024/60000 (28%)] Loss :1.857857\n",
      "Train Epoch : 1 [17088/60000 (28%)] Loss :1.790870\n",
      "Train Epoch : 1 [17408/60000 (29%)] Loss :1.759106\n",
      "Train Epoch : 1 [17472/60000 (29%)] Loss :1.728083\n",
      "Train Epoch : 1 [17536/60000 (29%)] Loss :1.814981\n",
      "Train Epoch : 1 [17600/60000 (29%)] Loss :1.746816\n",
      "Train Epoch : 1 [17920/60000 (30%)] Loss :1.783228\n",
      "Train Epoch : 1 [17984/60000 (30%)] Loss :1.772644\n",
      "Train Epoch : 1 [18048/60000 (30%)] Loss :1.751884\n",
      "Train Epoch : 1 [18112/60000 (30%)] Loss :1.684290\n",
      "Train Epoch : 1 [24576/60000 (41%)] Loss :1.326335\n",
      "Train Epoch : 1 [24640/60000 (41%)] Loss :1.285911\n",
      "Train Epoch : 1 [24704/60000 (41%)] Loss :1.349615\n",
      "Train Epoch : 1 [24768/60000 (41%)] Loss :1.324167\n",
      "Train Epoch : 1 [25088/60000 (42%)] Loss :1.242585\n",
      "Train Epoch : 1 [25152/60000 (42%)] Loss :1.248561\n",
      "Train Epoch : 1 [25216/60000 (42%)] Loss :1.239936\n",
      "Train Epoch : 1 [25280/60000 (42%)] Loss :1.236008\n",
      "Train Epoch : 1 [25600/60000 (43%)] Loss :1.201805\n",
      "Train Epoch : 1 [25664/60000 (43%)] Loss :1.206373\n",
      "Train Epoch : 1 [25728/60000 (43%)] Loss :1.227230\n",
      "Train Epoch : 1 [25792/60000 (43%)] Loss :1.202621\n",
      "Train Epoch : 1 [26112/60000 (43%)] Loss :1.245190\n",
      "Train Epoch : 1 [26176/60000 (44%)] Loss :1.194468\n",
      "Train Epoch : 1 [26240/60000 (44%)] Loss :1.083438\n",
      "Train Epoch : 1 [26304/60000 (44%)] Loss :1.226557\n",
      "Train Epoch : 1 [32768/60000 (55%)] Loss :0.780910\n",
      "Train Epoch : 1 [32832/60000 (55%)] Loss :0.860773\n",
      "Train Epoch : 1 [32896/60000 (55%)] Loss :0.832696\n",
      "Train Epoch : 1 [32960/60000 (55%)] Loss :0.885561\n",
      "Train Epoch : 1 [33280/60000 (55%)] Loss :0.994511\n",
      "Train Epoch : 1 [33344/60000 (56%)] Loss :0.861173\n",
      "Train Epoch : 1 [33408/60000 (56%)] Loss :0.776093\n",
      "Train Epoch : 1 [33472/60000 (56%)] Loss :0.856612\n",
      "Train Epoch : 1 [33792/60000 (56%)] Loss :0.941094\n",
      "Train Epoch : 1 [33856/60000 (56%)] Loss :0.775400\n",
      "Train Epoch : 1 [33920/60000 (57%)] Loss :0.848012\n",
      "Train Epoch : 1 [33984/60000 (57%)] Loss :0.857682\n",
      "Train Epoch : 1 [34304/60000 (57%)] Loss :0.808540\n",
      "Train Epoch : 1 [34368/60000 (57%)] Loss :0.820570\n",
      "Train Epoch : 1 [34432/60000 (57%)] Loss :0.846356\n",
      "Train Epoch : 1 [34496/60000 (57%)] Loss :0.742764\n",
      "Train Epoch : 1 [40960/60000 (68%)] Loss :0.547547\n",
      "Train Epoch : 1 [41024/60000 (68%)] Loss :0.599155\n",
      "Train Epoch : 1 [41088/60000 (68%)] Loss :0.619771\n",
      "Train Epoch : 1 [41152/60000 (69%)] Loss :0.565275\n",
      "Train Epoch : 1 [41472/60000 (69%)] Loss :0.632307\n",
      "Train Epoch : 1 [41536/60000 (69%)] Loss :0.628893\n",
      "Train Epoch : 1 [41600/60000 (69%)] Loss :0.812337\n",
      "Train Epoch : 1 [41664/60000 (69%)] Loss :0.655040\n",
      "Train Epoch : 1 [41984/60000 (70%)] Loss :0.499151\n",
      "Train Epoch : 1 [42048/60000 (70%)] Loss :0.601274\n",
      "Train Epoch : 1 [42112/60000 (70%)] Loss :0.495348\n",
      "Train Epoch : 1 [42176/60000 (70%)] Loss :0.624077\n",
      "Train Epoch : 1 [42496/60000 (71%)] Loss :0.828287\n",
      "Train Epoch : 1 [42560/60000 (71%)] Loss :0.559430\n",
      "Train Epoch : 1 [42624/60000 (71%)] Loss :0.677559\n",
      "Train Epoch : 1 [42688/60000 (71%)] Loss :0.700565\n",
      "Train Epoch : 1 [49152/60000 (82%)] Loss :0.503145\n",
      "Train Epoch : 1 [49216/60000 (82%)] Loss :0.396001\n",
      "Train Epoch : 1 [49280/60000 (82%)] Loss :0.430262\n",
      "Train Epoch : 1 [49344/60000 (82%)] Loss :0.522979\n",
      "Train Epoch : 1 [49664/60000 (83%)] Loss :0.541311\n",
      "Train Epoch : 1 [49728/60000 (83%)] Loss :0.710993\n",
      "Train Epoch : 1 [49792/60000 (83%)] Loss :0.496636\n",
      "Train Epoch : 1 [49856/60000 (83%)] Loss :0.455340\n",
      "Train Epoch : 1 [50176/60000 (84%)] Loss :0.418161\n",
      "Train Epoch : 1 [50240/60000 (84%)] Loss :0.472811\n",
      "Train Epoch : 1 [50304/60000 (84%)] Loss :0.426144\n",
      "Train Epoch : 1 [50368/60000 (84%)] Loss :0.429198\n",
      "Train Epoch : 1 [50688/60000 (84%)] Loss :0.508413\n",
      "Train Epoch : 1 [50752/60000 (85%)] Loss :0.386766\n",
      "Train Epoch : 1 [50816/60000 (85%)] Loss :0.663638\n",
      "Train Epoch : 1 [50880/60000 (85%)] Loss :0.491838\n",
      "Train Epoch : 1 [57344/60000 (96%)] Loss :0.450081\n",
      "Train Epoch : 1 [57408/60000 (96%)] Loss :0.425257\n",
      "Train Epoch : 1 [57472/60000 (96%)] Loss :0.447133\n",
      "Train Epoch : 1 [57536/60000 (96%)] Loss :0.427036\n",
      "Train Epoch : 1 [57856/60000 (96%)] Loss :0.408004\n",
      "Train Epoch : 1 [57920/60000 (96%)] Loss :0.406791\n",
      "Train Epoch : 1 [57984/60000 (97%)] Loss :0.479272\n",
      "Train Epoch : 1 [58048/60000 (97%)] Loss :0.419961\n",
      "Train Epoch : 1 [58368/60000 (97%)] Loss :0.523824\n",
      "Train Epoch : 1 [58432/60000 (97%)] Loss :0.381397\n",
      "Train Epoch : 1 [58496/60000 (97%)] Loss :0.496992\n",
      "Train Epoch : 1 [58560/60000 (98%)] Loss :0.437448\n",
      "Train Epoch : 1 [58880/60000 (98%)] Loss :0.390687\n",
      "Train Epoch : 1 [58944/60000 (98%)] Loss :0.468743\n",
      "Train Epoch : 1 [59008/60000 (98%)] Loss :0.342939\n",
      "Train Epoch : 1 [59072/60000 (98%)] Loss :0.502512\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.functional' has no attribute 'null_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1844c287330f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnull_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sum'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sum up batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m#get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# pred와 target 이 같은지 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn.functional' has no attribute 'null_loss'"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs +1):\n",
    "    #Train model\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device),  target.to(device)\n",
    "        optimizer.zero_grad() # backpropagation 계산하기 전에 0으로 기울기 계산\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward() # 계산한 기울기\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx & log_interval == 0:\n",
    "            print(\"Train Epoch : {} [{}/{} ({:.0f}%)] Loss :{:.6f}\".format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "            \n",
    "    #Test mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device) , target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.null_loss(output, target, reduction = 'sum' ).item() #sum up batch loss\n",
    "            pred = output.argmax(dim = 1 , keepdim = True ) #get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # pred와 target 이 같은지 확인\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    \n",
    "    print(\"Test set : Average loss : {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            test_loss, correct , len(test_loader.dataset),\n",
    "            100 * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
