{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets , transforms\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1  # shuffle 을 하더라도, seed 를 줘서 같은 결과가 나오도록 함 \n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "no_cuda = False\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('dataset', train = True , download = True,\n",
    "                      transform = transforms.Compose([  # argumentation 툴 - 노이즈를 줄수있는함수 등 다양한것을 한번에 처리해줌 \n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.1307,), (0.3081,))  # 정규화 - 여기서 전처리까지 같이 해서 나옴 \n",
    "                      ])),\n",
    "    batch_size = batch_size, shuffle = True )\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('dataset', train = False , transform= transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size = test_batch_size, shuffle = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):  # 이 클래스 안에 , 역전파까지 가능하게 들어가있음. 그래서 우린 어떤 모델 포워딩 할지만 정하면 됨 \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()  # 사용할 히든 스테이트를 정의\n",
    "        self.conv1 = nn.Conv2d(1, 20 , 5 , 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50 , 5, 1)\n",
    "        self.fc1 = nn.Linear( 4 *4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x ):  # 그 hidden state 가 어떻게 흘러가는지를 보여줌 \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim  =1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "- Model 과 Optimization 설정\n",
    "\n",
    "**확률적 경사 하강법 ( SGD , Stochastic Gradient Descent )**\n",
    "\n",
    ": 매개변수 값을 조정시 ( 매 스텝 ( step ) 에서 ), 전체 데이터가 아니라 랜덤으로 선택한 딱 1개의 샘플에 대해서 gradient를 계산한다.\n",
    "\n",
    "**모멘텀 ( Momentum )** \n",
    "\n",
    ": 관성이라는 물리학 법칙 처럼, 이동 벡터를 이용해 이전 **기울기**의 영향을 받도록 하는 것.\n",
    "\n",
    ": **이전 벡터의 이동의 크기를 현재에 반영해주는 것**이다.\n",
    "\n",
    "[관련해서 정리한 내용](https://hazel01.tistory.com/36?category=897675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device) # 모델을 불러옴\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.5 ) # 오티마이저를 불러옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parameters 들 확인\n",
    "    - Weight, Bais 를 순서대로 보여줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 5, 5])\n",
      "torch.Size([20])\n",
      "torch.Size([50, 20, 5, 5])\n",
      "torch.Size([50])\n",
      "torch.Size([500, 800])\n",
      "torch.Size([500])\n",
      "torch.Size([10, 500])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = list(model.parameters()) # 파라미터를 볼수 있음\n",
    "for i in range(8):\n",
    "    print(params[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Befor Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습하기 전에 Model이  Train 할 수 있도록  Train mode로 변환\n",
    "    - Convolution 또는 Linear 뿐만 아니라, dropout, batch normalization 과 같이 파라미터를 가진 레이어들도 학습하기 위해 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train() # trian mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델에 넣기 위한 첫 Batch 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, target = next(iter(train_loader))\n",
    "data.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 추출한 batch 데이터를 cpu 또는 gpu 와 같은 device에 compile \n",
    "     - 겉으로는 차이가 없지만, gpu를 할당받기 위해서? 여튼 compile 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, target = data.to(device), target.to(device)\n",
    "data.shape, target.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gradients를 clear해서 새로운 최적화 값을 찾기 위해 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()  # zero_grad를  이용하면, clear 해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 준비한 데이터를 model 에 input 으로 넣어 output을 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(data) # 이후, model에 넣어줌 keras는 보통 perdict , torch는 output이라 보통 씀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 에서 예측한 결과를 Loss Functoon 에 넣음\n",
    "    - 여기서는 Negative Log-likelihood loss 라는 loss function을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.nll_loss(output, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Back Propagation을 통해 Gradients를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 계산된 Gradients는 계산한 것으로 마무리 되는 것이 아니라 , Parameter 에 Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step() # update를 해주는 것. 계산된 결과를 넣어주는 것 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기까지가 원스텝! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "- 앞에서 model.train() 모드로 변한 것처럼 평가할 때는 model.eval()로 설정\n",
    "    - batch normaliztion 이나 drop out 같은 layer 들을 잠금\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- autograd engine, 즉 backpropagation 이나 gradient 계산 등을 껴서 memory usage 를 줄이고 속도를 높임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "correct  = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    data, target = next(iter(test_loader))\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model(data)\n",
    "    \n",
    "    # 계산용으로 loss 를 쌓음\n",
    "    test_loss += F.nll_loss(output, target, reduction= 'sum' ).item() #reduction 이걸 안하면, 배치사이즈를 기준으로 따로 계산해주는데, 이걸 sum 해주면 전체 데이터에 대해합쳐서 넣어줌\n",
    "    \n",
    "    pred = output.argmax(dim = 1 , keepdim = True ) # 차원수는 계속 유지\n",
    "    correct = pred.eq(target.view_as(pred)).sum() # True, False로 나온 값을 sum 함  - 같으게 얼마나 많은지 check 하는것 \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.667163848876953"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)\n",
    "print(pred.shape)\n",
    "print(target.view_as(pred).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90625"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한번 넣었을때(배치사이즈 ) 얼마나 맞췄는지 확인\n",
    "pred.eq(target.view_as(pred)).sum().item() / 64 # 총 이미지 갯수가 64 개 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2667163848876954e-11"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss /= len(test_loader.dataset)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch : 1 [0/60000 (0%)]   Loss : 0.235288\n",
      "Train Epoch : 1 [6400/60000 (11%)]   Loss : 0.203309\n",
      "Train Epoch : 1 [12800/60000 (21%)]   Loss : 0.178280\n",
      "Train Epoch : 1 [19200/60000 (32%)]   Loss : 0.242543\n",
      "Train Epoch : 1 [25600/60000 (43%)]   Loss : 0.264136\n",
      "Train Epoch : 1 [32000/60000 (53%)]   Loss : 0.170799\n",
      "Train Epoch : 1 [38400/60000 (64%)]   Loss : 0.277752\n",
      "Train Epoch : 1 [44800/60000 (75%)]   Loss : 0.113177\n",
      "Train Epoch : 1 [51200/60000 (85%)]   Loss : 0.139957\n",
      "Train Epoch : 1 [57600/60000 (96%)]   Loss : 0.276587\n",
      "Test set : Average Loss : 0.1952, Accuract : 9450/10000 (94%)\n",
      "Train Epoch : 2 [0/60000 (0%)]   Loss : 0.140250\n",
      "Train Epoch : 2 [6400/60000 (11%)]   Loss : 0.051344\n",
      "Train Epoch : 2 [12800/60000 (21%)]   Loss : 0.204236\n",
      "Train Epoch : 2 [19200/60000 (32%)]   Loss : 0.082684\n",
      "Train Epoch : 2 [25600/60000 (43%)]   Loss : 0.120147\n",
      "Train Epoch : 2 [32000/60000 (53%)]   Loss : 0.223981\n",
      "Train Epoch : 2 [38400/60000 (64%)]   Loss : 0.080957\n",
      "Train Epoch : 2 [44800/60000 (75%)]   Loss : 0.092156\n",
      "Train Epoch : 2 [51200/60000 (85%)]   Loss : 0.079063\n",
      "Train Epoch : 2 [57600/60000 (96%)]   Loss : 0.103186\n",
      "Test set : Average Loss : 0.1631, Accuract : 9529/10000 (95%)\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "log_interval = 100  # 몇번 로그만에 보여줄지 \n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    for batch_idx,  (data, target ) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device) # 데이터를 뽑아줌 \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        # -------- 여기까지가 한 스텝임  ------- #\n",
    "         \n",
    "        # 잘 진행되고 있는지 확인하기 위한코드\n",
    "        if  batch_idx % log_interval == 0 :\n",
    "            print(\"Train Epoch : {} [{}/{} ({:.0f}%)]   Loss : {:.6f}\".format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),  # 전체 데이터 셋\n",
    "            100* batch_idx / len(train_loader), loss.item()\n",
    "            ))\n",
    "            \n",
    "            \n",
    "    model.eval() # 한 에폭에 따른 평가\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction = 'sum').item()\n",
    "            pred = output.argmax(dim  =1 , keepdim = True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('Test set : Average Loss : {:.4f}, Accuract : {}/{} ({:.0f}%)'.format(\n",
    "            test_loss, correct, len(test_loader.dataset), 100* correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
