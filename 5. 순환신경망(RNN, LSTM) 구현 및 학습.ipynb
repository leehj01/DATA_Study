{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순환 신경망 구현 및 학습\n",
    "- tensorflow 를 이용하여 순환 신경망 구현할 것! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "NUM_WORDS = 10000 # 분석을 하기위해서 만개의 단어만 사용하겠다는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델정의\n",
    "- 여기서 SimplerRNN -> LSTM 으로 바꾸어주면, LSTM으로 바뀌게 됨. 혹은 GRU로 바꾸면 GRU가 됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 16) \n",
    "        # 원핫 벡터가 10000일때, 첫번째가 MY 였다면, 특정 단어들이 원핫 벡터들이 하나씩 표현됨. \n",
    "        # Embedding : 원핫 벡터는 정수값이라서, 심지어 0,1의 바이너리값이여서, 실수값을 가져오기 위해서 길이가 10000인 원핫 벡터를 길이가 16인 피쳐 벡터로 바꿔주는 역할을 함\n",
    "        \n",
    "        self.rnn = tf.keras.layers.SimpleRNN(32) # rnn layer를 사용하는데, 심플하게 바닐라 rnn사용\n",
    "        # 여기서 SimplerRNN -> LSTM 으로 바꾸어주면, LSTM으로 바뀌게 됨. 혹은 GRU로 바꾸면 GRU가 됨 \n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(2, activation = 'softmax') # 감정분석 을 할거기 때문에, 길이가 2인 소프트 맥스를 사용 \n",
    "        \n",
    "    def call(self, x , training = None, mask = None):\n",
    "        x = self.emb(x)\n",
    "        x = self.rnn(x)\n",
    "        return self.dense(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM은 히든 레이러를 늘리면 성능이 더 안좋아 진다고 한다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 16) \n",
    "        \n",
    "        self.rnn1 = tf.keras.layers.LSTM(32) \n",
    "        self.rnn2 = tf.keras.layers.LSTM(32)\n",
    "        self.rnn3 = tf.keras.layers.LSTM(32)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(2, activation = 'softmax') \n",
    "        \n",
    "    def call(self, x , training = None, mask = None):\n",
    "        x = self.emb(x)\n",
    "        x = self.rnn(x)\n",
    "        return self.dense(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습, 테스트 루프 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement training loop \n",
    "\n",
    "@tf.function\n",
    "def train_step(model, inputs , labels, loss_object, optimizer, train_loss, train_accuracy ) :\n",
    "    with tf.GradientTape() as tape :\n",
    "        predictions = model(inputs, training = True )\n",
    "        loss = loss_object(labels, predictions )\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions )\n",
    "    \n",
    "\n",
    "# Implement algorithm test\n",
    "@tf.function \n",
    "def test_step(model, inputs, labels, loss_object, test_loss, test_accuracy):\n",
    "    predictions = model(inputs, training = False)\n",
    "    \n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 셋 준비\n",
    "- Y가 다중 출력인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = tf.keras.datasets.imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = NUM_WORDS)\n",
    "\n",
    "\n",
    "# 데이터의 L 의 길이가 다르기 때문에, PRE -PADDING 을 해준다. \n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, \n",
    "                                                        value = 0,  # 0으로 패팅을 해줌\n",
    "                                                        padding = 'pre',\n",
    "                                                        maxlen = 32) # 최대길이를 32로 잘라주면서 앞에 패팅 0해줌\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, \n",
    "                                                        value = 0,  # 0으로 패팅을 해줌\n",
    "                                                        padding = 'pre',\n",
    "                                                        maxlen = 32) # 최대길이를 32로 잘라주면서 앞에 패팅 0해줌\n",
    "\n",
    "# 학습할 때마다 셔플이 일어날 수 있도록 설정 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 환경 정의\n",
    "### 모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = MyModel()\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define performance metrics\n",
    "train_loss = tf.keras.metrics.Mean(name = 'train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name= 'train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습루프 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 , loss : 0.5539466142654419, Accuracy : 69.33200073242188, Test Loss : 0.45906075835227966 , Test Accuracy : 78.1240005493164\n",
      "Epoch 2 , loss : 0.352559894323349, Accuracy : 84.69999694824219, Test Loss : 0.49132290482521057 , Test Accuracy : 77.33200073242188\n",
      "Epoch 3 , loss : 0.20189930498600006, Accuracy : 92.16400146484375, Test Loss : 0.6088957786560059 , Test Accuracy : 76.08799743652344\n",
      "Epoch 4 , loss : 0.08653007447719574, Accuracy : 97.14400482177734, Test Loss : 0.8594751358032227 , Test Accuracy : 74.70399475097656\n",
      "Epoch 5 , loss : 0.03685051575303078, Accuracy : 98.8239974975586, Test Loss : 1.0452150106430054 , Test Accuracy : 73.63600158691406\n",
      "Epoch 6 , loss : 0.016997288912534714, Accuracy : 99.49200439453125, Test Loss : 1.2401323318481445 , Test Accuracy : 74.05599975585938\n",
      "Epoch 7 , loss : 0.01626214012503624, Accuracy : 99.49200439453125, Test Loss : 1.376662254333496 , Test Accuracy : 74.51599884033203\n",
      "Epoch 8 , loss : 0.016662398353219032, Accuracy : 99.447998046875, Test Loss : 1.478561520576477 , Test Accuracy : 73.8239974975586\n",
      "Epoch 9 , loss : 0.018502414226531982, Accuracy : 99.39600372314453, Test Loss : 1.4707642793655396 , Test Accuracy : 73.61199951171875\n",
      "Epoch 10 , loss : 0.013434432446956635, Accuracy : 99.552001953125, Test Loss : 1.4981067180633545 , Test Accuracy : 73.40399932861328\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for inputs , labels in train_dataset:\n",
    "        train_step(model, inputs , labels, loss_object, optimizer, train_loss, train_accuracy)\n",
    "        \n",
    "    for test_seqs , test_labels in test_dataset:\n",
    "        test_step(model, test_seqs, test_labels, loss_object, test_loss, test_accuracy)\n",
    "        \n",
    "    template = 'Epoch {} , loss : {}, Accuracy : {}, Test Loss : {} , Test Accuracy : {}'\n",
    "    print( template.format(epoch +1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result()*100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result()*100))\n",
    "    \n",
    "    train_loss.reset_states() # 다음 훈련할 데이터가 이전의 훈련할 데이터와 완전히 연관 없을 때 사용하는 것이다\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
